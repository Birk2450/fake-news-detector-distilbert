{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:52:50.932038Z",
     "start_time": "2025-11-17T12:52:50.928999Z"
    }
   },
   "source": [
    "# ======== CONFIGURATION CELL ========\n",
    "\n",
    "# When running configs, change the values in this cell, restart kernel, and re-run all subsequentcells\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 256,\n",
    "}\n",
    "print(\"Current configuration:\", CONFIG)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration: {'model_name': 'distilbert-base-uncased', 'learning_rate': 2e-05, 'batch_size': 16, 'num_epochs': 3, 'max_length': 256}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:52:56.302828Z",
     "start_time": "2025-11-17T12:52:55.911728Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# confirm VS Code sees your data folder:\n",
    "print(os.listdir(\"../data\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fake.csv', 'val.csv', 'test.csv', 'merged_dataset.csv', 'README.md', 'train.csv', 'True.csv']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:53:00.851734Z",
     "start_time": "2025-11-17T12:52:59.245974Z"
    }
   },
   "source": [
    "# Load pre-split data\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "val_df   = pd.read_csv(\"../data/val.csv\")\n",
    "test_df  = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  SocGen says no wrongdoing in handling of Natio...   \n",
       "1  North Carolina governor concedes election to D...   \n",
       "2  TRUMP FEVER! W. VA Dem Senator Says He Won’t V...   \n",
       "3  New York vows to sue Trump over immigrant chil...   \n",
       "4  Orlando killer expressed support for multiple ...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  PARIS (Reuters) - French bank Societe Generale...     worldnews   \n",
       "1  WINSTON-SALEM, N.C. (Reuters) - North Carolina...  politicsNews   \n",
       "2  Civil political discourse took a beating in We...     left-news   \n",
       "3  (Reuters) - New York and Washington state on M...  politicsNews   \n",
       "4  ORLANDO, Fla. (Reuters) - Orlando nightclub ki...  politicsNews   \n",
       "\n",
       "                 date  label  \\\n",
       "0  November 22, 2017       1   \n",
       "1   December 5, 2016       1   \n",
       "2         Aug 7, 2017      0   \n",
       "3  September 4, 2017       1   \n",
       "4      June 12, 2016       1   \n",
       "\n",
       "                                           text_full  \n",
       "0  SocGen says no wrongdoing in handling of Natio...  \n",
       "1  North Carolina governor concedes election to D...  \n",
       "2  TRUMP FEVER! W. VA Dem Senator Says He Won’t V...  \n",
       "3  New York vows to sue Trump over immigrant chil...  \n",
       "4  Orlando killer expressed support for multiple ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>text_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SocGen says no wrongdoing in handling of Natio...</td>\n",
       "      <td>PARIS (Reuters) - French bank Societe Generale...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 22, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>SocGen says no wrongdoing in handling of Natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Carolina governor concedes election to D...</td>\n",
       "      <td>WINSTON-SALEM, N.C. (Reuters) - North Carolina...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 5, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>North Carolina governor concedes election to D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRUMP FEVER! W. VA Dem Senator Says He Won’t V...</td>\n",
       "      <td>Civil political discourse took a beating in We...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Aug 7, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUMP FEVER! W. VA Dem Senator Says He Won’t V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New York vows to sue Trump over immigrant chil...</td>\n",
       "      <td>(Reuters) - New York and Washington state on M...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 4, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>New York vows to sue Trump over immigrant chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orlando killer expressed support for multiple ...</td>\n",
       "      <td>ORLANDO, Fla. (Reuters) - Orlando nightclub ki...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 12, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Orlando killer expressed support for multiple ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:53:02.771205Z",
     "start_time": "2025-11-17T12:53:02.763480Z"
    }
   },
   "source": [
    "# Our input text and labels\n",
    "X_train_text = train_df[\"text_full\"].astype(str).tolist()\n",
    "y_train      = train_df[\"label\"].tolist()\n",
    "\n",
    "X_val_text   = val_df[\"text_full\"].astype(str).tolist()\n",
    "y_val        = val_df[\"label\"].tolist()\n",
    "\n",
    "X_test_text  = test_df[\"text_full\"].astype(str).tolist()\n",
    "y_test       = test_df[\"label\"].tolist()\n",
    "\n",
    "len(X_train_text), len(X_val_text), len(X_test_text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35918, 4490, 4490)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:07:47.636121Z",
     "start_time": "2025-11-17T12:53:05.829765Z"
    }
   },
   "source": [
    "################################################### NOTES ###################################################\n",
    "# We use our preprocessed CSVs → X_train_text, y_train, etc.\n",
    "# No more transformed_text_title_combined or Kaggle paths.\n",
    "# DistilBERT sees: text_full (title + body) and label (0 = fake, 1 = real).\n",
    "# We use train + val for training/validation; test stays untouched for final evaluation.\n",
    "#############################################################################################################\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable Weights & Biases spam\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 1. Load tokenizer (from CONFIG)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# 2. Convert our lists -> Hugging Face Dataset objects\n",
    "train_ds = Dataset.from_dict({\"text\": X_train_text, \"label\": y_train})\n",
    "val_ds   = Dataset.from_dict({\"text\": X_val_text,   \"label\": y_val})\n",
    "test_ds  = Dataset.from_dict({\"text\": X_test_text,  \"label\": y_test})\n",
    "\n",
    "# 3. Tokenization\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_val   = val_ds.map(tokenize_function,   batched=True)\n",
    "tokenized_test  = test_ds.map(tokenize_function,  batched=True)\n",
    "\n",
    "# 4. Load pre-trained DistilBERT for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    num_labels=2,  # 0 = Fake, 1 = Real\n",
    ")\n",
    "\n",
    "# 5. Data collator (handles padding dynamically)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 6. Training arguments (all from CONFIG)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # use validation set each epoch\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"../logs\",\n",
    ")\n",
    "\n",
    "# 7. Trainer – IMPORTANT: use validation set as eval_dataset, not test\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 8. Train (fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "# 9. Save fine-tuned model\n",
    "trainer.save_model(\"../models/distilbert_finetuned\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/35918 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7c1583835da4f028031525488e9d44e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4490 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed368f4e7dc04226943ded2a7fec6714"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4490 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f851647a90b4cca96e524d6029e74e0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/var/folders/09/f1bm6mk51zn1sc1fthq_md9m0000gn/T/ipykernel_94715/233075719.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/agust/Desktop/Agustin/UPM/Repositories UPM/Milestone/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2273' max='6735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2273/6735 14:23 < 28:16, 2.63 it/s, Epoch 1.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.001356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "72cf3b05ecc3d01414e1ddb034649fd5"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agust/Desktop/Agustin/UPM/Repositories UPM/Milestone/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 80\u001B[39m\n\u001B[32m     70\u001B[39m trainer = Trainer(\n\u001B[32m     71\u001B[39m     model=model,\n\u001B[32m     72\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     76\u001B[39m     data_collator=data_collator,\n\u001B[32m     77\u001B[39m )\n\u001B[32m     79\u001B[39m \u001B[38;5;66;03m# 8. Train (fine-tuning)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[38;5;66;03m# 9. Save fine-tuned model\u001B[39;00m\n\u001B[32m     83\u001B[39m trainer.save_model(\u001B[33m\"\u001B[39m\u001B[33m../models/distilbert_finetuned\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Agustin/UPM/Repositories UPM/Milestone/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Agustin/UPM/Repositories UPM/Milestone/.venv/lib/python3.13/site-packages/transformers/trainer.py:2679\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m   2674\u001B[39m     tr_loss_step = \u001B[38;5;28mself\u001B[39m.training_step(model, inputs, num_items_in_batch)\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m-> \u001B[39m\u001B[32m2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43misinf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss_step\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n\u001B[32m   2683\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/hhp6r0ys2bnc28g7hvlfhqg40000gn/T/ipykernel_6484/4288862160.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "/Users/birkbregendahl/Desktop/ITU/SD3-ETSIINF/Deep Learning And Software Engineering/Projects/DL_Group_Project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results:\n",
      "{'eval_loss': 4.3559000914683565e-05, 'eval_model_preparation_time': 0.004, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 41.6157, 'eval_samples_per_second': 107.892, 'eval_steps_per_second': 6.752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birkbregendahl/Desktop/ITU/SD3-ETSIINF/Deep Learning And Software Engineering/Projects/DL_Group_Project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results:\n",
      "{'eval_loss': 0.0003635552420746535, 'eval_model_preparation_time': 0.004, 'eval_accuracy': 0.9997772828507795, 'eval_precision': 1.0, 'eval_recall': 0.9995331465919701, 'eval_f1': 0.999766518795237, 'eval_runtime': 40.2702, 'eval_samples_per_second': 111.497, 'eval_steps_per_second': 6.978}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# Re-create a Trainer JUST for evaluation, using the trained model\n",
    "eval_trainer = Trainer(\n",
    "    model=model,                  # this is the fine-tuned model from Cell 5\n",
    "    args=training_args,           # same TrainingArguments\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,   # default eval dataset = validation set\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 1) Evaluate on validation set\n",
    "val_results = eval_trainer.evaluate()\n",
    "print(\"Validation results:\")\n",
    "print(val_results)\n",
    "\n",
    "# 2) Evaluate on test set (explicitly pass tokenized_test)\n",
    "test_results = eval_trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"\\nTest results:\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Government announces new education reform to support low-income students.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Shocking! Scientists prove that drinking only coffee for a week makes you immortal.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Major tech company releases open-source AI model for medical diagnosis.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Experts claim that the moon will crash into Earth next year according to secret documents.\n",
      "Prediction: Fake\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model + tokenizer from disk\n",
    "model_path = \"../models/distilbert_finetuned\"\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "# Label mapping: 0 = Fake, 1 = Real (our convention)\n",
    "label_names = [\"Fake\", \"Real\"]\n",
    "\n",
    "# Example news texts (you can change these to whatever you like) (ChatGPT's idea)\n",
    "sample_texts = [\n",
    "    \"Government announces new education reform to support low-income students.\",\n",
    "    \"Shocking! Scientists prove that drinking only coffee for a week makes you immortal.\",\n",
    "    \"Major tech company releases open-source AI model for medical diagnosis.\",\n",
    "    \"Experts claim that the moon will crash into Earth next year according to secret documents.\",\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Prediction:\", label_names[predicted_class_id])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL example pred: Real\n",
      "FAKE example pred: Fake\n"
     ]
    }
   ],
   "source": [
    "# Pick one known real and one known fake from your data\n",
    "real_example = true_example = train_df[train_df[\"label\"] == 1][\"text_full\"].iloc[0]\n",
    "fake_example = train_df[train_df[\"label\"] == 0][\"text_full\"].iloc[0]\n",
    "\n",
    "def predict_text(text):\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        pred_id = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return label_names[pred_id]\n",
    "\n",
    "print(\"REAL example pred:\", predict_text(real_example))\n",
    "print(\"FAKE example pred:\", predict_text(fake_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Prepare a summary of this experiment\n",
    "experiment_row = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"learning_rate\": CONFIG[\"learning_rate\"],\n",
    "    \"batch_size\": CONFIG[\"batch_size\"],\n",
    "    \"num_epochs\": CONFIG[\"num_epochs\"],\n",
    "    \"max_length\": CONFIG[\"max_length\"],\n",
    "    \"val_accuracy\": val_results.get(\"eval_accuracy\", None),\n",
    "    \"val_f1\": val_results.get(\"eval_f1\", None),\n",
    "    \"test_accuracy\": test_results.get(\"eval_accuracy\", None),\n",
    "    \"test_f1\": test_results.get(\"eval_f1\", None)\n",
    "}\n",
    "\n",
    "# Create log file if it doesn't exist\n",
    "log_path = \"../experiment_results.csv\"\n",
    "file_exists = os.path.isfile(log_path)\n",
    "\n",
    "with open(log_path, \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=experiment_row.keys())\n",
    "    \n",
    "    if not file_exists:\n",
    "        writer.writeheader()   # first time: write column names\n",
    "\n",
    "    writer.writerow(experiment_row)\n",
    "\n",
    "print(\"Experiment logged!\")\n",
    "print(experiment_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:58:55.023087Z",
     "start_time": "2025-12-03T16:58:55.017807Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======== CONFIGURATION CELL ========\n",
    "# 6 experiment configurations varying: learning_rate, num_epochs\n",
    "\n",
    "EXPERIMENT_CONFIGS = [\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-1\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_epochs\": 1\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-2\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_epochs\": 2\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-3\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_epochs\": 3\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-4\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_epochs\": 1\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-5\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_epochs\": 2\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CONFIGURATION-6\",\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_epochs\": 3\n",
    "    },\n",
    "]\n",
    "\n",
    "# How many repetitions per configuration\n",
    "NUM_RUNS_PER_CONFIG = 5\n",
    "\n",
    "# Please modify this field with your name / machine id\n",
    "MACHINE_ID = \"PC_Birk\"   # Karol ‚Üí \"PC_Karol\", etc.\n",
    "\n",
    "print(\"Configured experiments:\")\n",
    "for cfg in EXPERIMENT_CONFIGS:\n",
    "    print(f\"  {cfg['id']}: lr={cfg['learning_rate']}, epochs={cfg['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:58:56.862892Z",
     "start_time": "2025-12-03T16:58:56.859314Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# confirm VS Code sees your data folder:\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:58:59.570090Z",
     "start_time": "2025-12-03T16:58:57.998101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-split data\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "val_df   = pd.read_csv(\"../data/val.csv\")\n",
    "test_df  = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:59:04.225263Z",
     "start_time": "2025-12-03T16:59:01.033304Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Simple sanity check: make sure Reuters source tags were removed in preprocessing\n",
    "pattern = r'[\\(\\[]\\s*Reuters\\s*[\\)\\]]|^\\s*Reuters\\s*-\\s*'\n",
    "\n",
    "\n",
    "def check_reuters(df, split_name):\n",
    "    cols_to_check = [c for c in [\"text_full\", \"text\", \"title\"] if c in df.columns]\n",
    "    total = 0\n",
    "    for col in cols_to_check:\n",
    "        count = df[col].astype(str).str.contains(pattern, regex=True).sum()\n",
    "        print(f\"{split_name}: {count} rows still contain Reuters-tag pattern in '{col}'\")\n",
    "        total += count\n",
    "    if total == 0:\n",
    "        print(f\"‚úÖ No Reuters source tags found in {split_name} split.\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Found {total} Reuters-tagged rows in {split_name}.\\n\"\n",
    "              f\"   ‚Üí You may need to re-run preprocessing.py to regenerate the CSVs.\\n\")\n",
    "\n",
    "\n",
    "check_reuters(train_df, \"train\")\n",
    "check_reuters(val_df, \"val\")\n",
    "check_reuters(test_df, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:59:06.429602Z",
     "start_time": "2025-12-03T16:59:06.405226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our input text and labels\n",
    "X_train_text = train_df[\"text_full\"].astype(str).tolist()\n",
    "y_train      = train_df[\"label\"].tolist()\n",
    "\n",
    "X_val_text   = val_df[\"text_full\"].astype(str).tolist()\n",
    "y_val        = val_df[\"label\"].tolist()\n",
    "\n",
    "X_test_text  = test_df[\"text_full\"].astype(str).tolist()\n",
    "y_test       = test_df[\"label\"].tolist()\n",
    "\n",
    "len(X_train_text), len(X_val_text), len(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:59:08.960678Z",
     "start_time": "2025-12-03T16:59:08.957691Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:59:20.714505Z",
     "start_time": "2025-12-03T16:59:12.989978Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################### NOTES ###################################################\n",
    "# We use our preprocessed CSVs ‚Üí X_train_text, y_train, etc.\n",
    "# No more transformed_text_title_combined or Kaggle paths.\n",
    "# DistilBERT sees: text_full (title + body) and label (0 = fake, 1 = real).\n",
    "# We use train + val for training/validation; test stays untouched for final evaluation.\n",
    "#############################################################################################################\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable Weights & Biases spam\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Prefer MPS (Apple Silicon), then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Final device: {device}\")\n",
    "\n",
    "\n",
    "# 1. Load tokenizer (from CONFIG)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# 2. Convert our lists -> Hugging Face Dataset objects\n",
    "train_ds = Dataset.from_dict({\"text\": X_train_text, \"label\": y_train})\n",
    "val_ds   = Dataset.from_dict({\"text\": X_val_text,   \"label\": y_val})\n",
    "test_ds  = Dataset.from_dict({\"text\": X_test_text,  \"label\": y_test})\n",
    "\n",
    "# 3. Tokenization\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_val   = val_ds.map(tokenize_function,   batched=True)\n",
    "tokenized_test  = test_ds.map(tokenize_function,  batched=True)\n",
    "\n",
    "\n",
    "# 4. Data collator (handles padding dynamically)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-03T16:59:24.011813Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import csv\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "log_csv_path = \"../experiment_results.csv\"\n",
    "\n",
    "# Root folder para los TXT legibles\n",
    "txt_logs_root = \"../run_logs_results\"\n",
    "os.makedirs(txt_logs_root, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "csv_fieldnames = [\n",
    "    \"timestamp\",\n",
    "    \"experiment_id\",\n",
    "    \"run_index\",\n",
    "    \"model_name\",\n",
    "    \"machine_id\",\n",
    "    \"learning_rate\",\n",
    "    \"num_epochs\",\n",
    "    \"val_accuracy\",\n",
    "    \"val_precision\",\n",
    "    \"val_recall\",\n",
    "    \"val_f1\",\n",
    "]\n",
    "\n",
    "csv_exists = os.path.isfile(log_csv_path)\n",
    "if not csv_exists:\n",
    "    with open(log_csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=csv_fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "# =========================\n",
    "# NEW: loop over configs\n",
    "# =========================\n",
    "for CONFIG in EXPERIMENT_CONFIGS:\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(f\"STARTING {CONFIG['id']}  (lr={CONFIG['learning_rate']}, \"\n",
    "          f\"epochs={CONFIG['num_epochs']})\")\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    for run_idx in range(NUM_RUNS_PER_CONFIG):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"RUN {run_idx + 1}/{NUM_RUNS_PER_CONFIG}  |  {CONFIG['id']}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 0) Different Seed per run\n",
    "        seed_value = 42 + run_idx\n",
    "        set_seed(seed_value)\n",
    "\n",
    "        # 1) Reinitialize model from DistilBERT base\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"model_name\"],\n",
    "            num_labels=2,\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        # 2) Directorios temporales para checkpoints/logs de este run\n",
    "        run_output_dir = f\"../results/{CONFIG['id']}/run_{run_idx + 1}\"\n",
    "        run_logging_dir = f\"../logs/{CONFIG['id']}/run_{run_idx + 1}\"\n",
    "\n",
    "        # 3) TrainingArguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=run_output_dir,\n",
    "            learning_rate=CONFIG[\"learning_rate\"],\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "            logging_dir=run_logging_dir,\n",
    "            report_to=[],\n",
    "            seed=seed_value,\n",
    "            eval_strategy=\"no\",\n",
    "            save_strategy=\"no\",\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"eval_f1\",\n",
    "            logging_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        # 4) Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        # 5) Training (the model in the end is the one of the last epoch)\n",
    "        trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # 6) Evaluate only on the validation with the model of the last epoch.\n",
    "        val_results = trainer.evaluate()\n",
    "\n",
    "        def pretty_print_metrics(name, metrics):\n",
    "            print(f\"\\n{name} metrics:\")\n",
    "            print(f\"  Accuracy : {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\"  Precision: {metrics.get('eval_precision', 0):.4f}\")\n",
    "            print(f\"  Recall   : {metrics.get('eval_recall', 0):.4f}\")\n",
    "            print(f\"  F1-score : {metrics.get('eval_f1', 0):.4f}\")\n",
    "\n",
    "        pretty_print_metrics(\"Validation\", val_results)\n",
    "        print()\n",
    "\n",
    "        # 7) Save the model of this run (last epoch) in a stable folder.\n",
    "        final_model_dir = os.path.join(\"../models\", CONFIG[\"id\"], f\"run_{run_idx + 1}\")\n",
    "        os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(final_model_dir)\n",
    "        tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "        print(f\"üíæ Last epoch model for this run: {final_model_dir}\")\n",
    "\n",
    "        # 7.1) Delete temporary folder of results (to maintain clean)\n",
    "        shutil.rmtree(run_output_dir, ignore_errors=True)\n",
    "\n",
    "        # 8) Log a CSV immediately after finishing the run.\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        row = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"experiment_id\": CONFIG[\"id\"],\n",
    "            \"run_index\": run_idx + 1,\n",
    "            \"model_name\": CONFIG[\"model_name\"],\n",
    "            \"machine_id\": MACHINE_ID,\n",
    "            \"learning_rate\": CONFIG[\"learning_rate\"],\n",
    "            \"num_epochs\": CONFIG[\"num_epochs\"],\n",
    "            \"val_accuracy\": val_results.get(\"eval_accuracy\"),\n",
    "            \"val_precision\": val_results.get(\"eval_precision\"),\n",
    "            \"val_recall\": val_results.get(\"eval_recall\"),\n",
    "            \"val_f1\": val_results.get(\"eval_f1\"),\n",
    "        }\n",
    "\n",
    "        with open(log_csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=csv_fieldnames)\n",
    "            writer.writerow(row)\n",
    "\n",
    "        print(\"üìù Logged validation metrics to experiment_results.csv\")\n",
    "\n",
    "        # 9) Log TXT legible por run\n",
    "        config_logs_dir = os.path.join(txt_logs_root, CONFIG[\"id\"])\n",
    "        os.makedirs(config_logs_dir, exist_ok=True)\n",
    "\n",
    "        txt_path = os.path.join(config_logs_dir, f\"run_{run_idx + 1}.txt\")\n",
    "        with open(txt_path, \"w\") as f:\n",
    "            f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "            f.write(f\"Experiment ID: {CONFIG['id']}\\n\")\n",
    "            f.write(f\"Run index: {run_idx + 1}\\n\")\n",
    "            f.write(f\"Model: {CONFIG['model_name']}\\n\")\n",
    "            f.write(f\"Machine: {MACHINE_ID}\\n\")\n",
    "            f.write(f\"Learning rate: {CONFIG['learning_rate']}\\n\")\n",
    "            f.write(f\"Epochs: {CONFIG['num_epochs']}\\n\")\n",
    "\n",
    "            f.write(\"Validation metrics:\\n\")\n",
    "            f.write(f\"  Accuracy:  {val_results.get('eval_accuracy')}\\n\")\n",
    "            f.write(f\"  Precision: {val_results.get('eval_precision')}\\n\")\n",
    "            f.write(f\"  Recall:    {val_results.get('eval_recall')}\\n\")\n",
    "            f.write(f\"  F1:        {val_results.get('eval_f1')}\\n\")\n",
    "\n",
    "        print(f\"üìÑ Saved TXT log to: {txt_path}\")\n",
    "        print(f\"‚úÖ Finished run {run_idx + 1}/{NUM_RUNS_PER_CONFIG} for {CONFIG['id']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T15:29:41.140879Z",
     "start_time": "2025-11-25T15:29:41.122765Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all logged runs\n",
    "results = pd.read_csv(\"../experiment_results.csv\")\n",
    "\n",
    "# Group by configuration + hyperparams\n",
    "group_cols = [\n",
    "    \"experiment_id\",\n",
    "    \"model_name\",\n",
    "    \"learning_rate\",\n",
    "    \"num_epochs\",\n",
    "]\n",
    "\n",
    "summary = (\n",
    "    results\n",
    "    .groupby(group_cols)\n",
    "    .agg(\n",
    "        runs=(\"run_index\", \"nunique\"),\n",
    "        val_accuracy_mean=(\"val_accuracy\", \"mean\"),\n",
    "        val_accuracy_std=(\"val_accuracy\", \"std\"),\n",
    "        val_f1_mean=(\"val_f1\", \"mean\"),\n",
    "        val_f1_std=(\"val_f1\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"experiment_id\"])\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T15:38:48.391820Z",
     "start_time": "2025-11-25T15:38:48.282798Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple bar plot: mean Test F1 per configuration\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(summary[\"experiment_id\"], summary[\"test_f1_mean\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Mean Test F1\")\n",
    "plt.title(\"Mean Test F1 score per configuration\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: bar plot for Validation Accuracy\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(summary[\"experiment_id\"], summary[\"val_accuracy_mean\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Mean Validation Accuracy\")\n",
    "plt.title(\"Mean Validation accuracy per configuration\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T15:39:21.331438Z",
     "start_time": "2025-11-25T15:38:51.803590Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "# 1) Load all the results\n",
    "results = pd.read_csv(\"../experiment_results.csv\")\n",
    "\n",
    "# 2) Filter specifically this configuration.\n",
    "config_id = CONFIG[\"id\"]\n",
    "config_rows = results[results[\"experiment_id\"] == config_id]\n",
    "\n",
    "# 3) Choose the run with the best F1 in Validation\n",
    "best_row = config_rows.sort_values(\"val_f1\", ascending=False).iloc[0]\n",
    "best_run = int(best_row[\"run_index\"])\n",
    "\n",
    "print(f\"üèÜ Best run for {config_id} based on validation F1:\")\n",
    "display(best_row)\n",
    "\n",
    "# 4) Load that model\n",
    "best_model_dir = os.path.join(\"../models\", config_id, f\"run_{best_run}\")\n",
    "print(f\"\\nLoading best model from: {best_model_dir}\")\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
    "best_model.to(device)\n",
    "\n",
    "# 5) Create a Trainer only for evaluation in test.\n",
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_best_eval\",\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "best_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=test_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_results = best_trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä FINAL TEST METRICS (single evaluation for this configuration):\")\n",
    "print(f\"  Accuracy : {test_results.get('eval_accuracy', 0):.4f}\")\n",
    "print(f\"  Precision: {test_results.get('eval_precision', 0):.4f}\")\n",
    "print(f\"  Recall   : {test_results.get('eval_recall', 0):.4f}\")\n",
    "print(f\"  F1-score : {test_results.get('eval_f1', 0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T15:41:12.186145Z",
     "start_time": "2025-11-25T15:41:11.968040Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model + tokenizer from disk\n",
    "model_path = \"../models/distilbert_finetuned\"\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "# Label mapping: 0 = Fake, 1 = Real (our convention)\n",
    "label_names = [\"Fake\", \"Real\"]\n",
    "\n",
    "# Example news texts (you can change these to whatever you like) (ChatGPT's idea)\n",
    "sample_texts = [\n",
    "    \"Government announces new education reform to support low-income students.\",\n",
    "    \"Shocking! Scientists prove that drinking only coffee for a week makes you immortal.\",\n",
    "    \"Major tech company releases open-source AI model for medical diagnosis.\",\n",
    "    \"Experts claim that the moon will crash into Earth next year according to secret documents.\",\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Prediction:\", label_names[predicted_class_id])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T15:41:31.917718Z",
     "start_time": "2025-11-25T15:41:31.828392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pick one known real and one known fake from your data\n",
    "real_example = true_example = train_df[train_df[\"label\"] == 1][\"text_full\"].iloc[0]\n",
    "fake_example = train_df[train_df[\"label\"] == 0][\"text_full\"].iloc[0]\n",
    "\n",
    "def predict_text(text):\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        pred_id = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return label_names[pred_id]\n",
    "\n",
    "print(\"REAL example pred:\", predict_text(real_example))\n",
    "print(\"FAKE example pred:\", predict_text(fake_example))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL Group Project",
   "language": "python",
   "name": "dl_group_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
